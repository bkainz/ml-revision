<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Machine Learning Revision</title>
<!-- 2017-01-10 Tue 10:17 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Tom Eccles" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Machine Learning Revision</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. About This Document</a></li>
<li><a href="#sec-2">2. Maths Review</a>
<ul>
<li><a href="#sec-2-1">2.1. Matrices and Vectors</a>
<ul>
<li><a href="#sec-2-1-1">2.1.1. Basics</a></li>
<li><a href="#sec-2-1-2">2.1.2. Linear Transformation</a></li>
</ul>
</li>
<li><a href="#sec-2-2">2.2. Calculus</a>
<ul>
<li><a href="#sec-2-2-1">2.2.1. Basics</a></li>
<li><a href="#sec-2-2-2">2.2.2. Optimisation</a></li>
</ul>
</li>
<li><a href="#sec-2-3">2.3. Probability</a>
<ul>
<li><a href="#sec-2-3-1">2.3.1. Bayes Theorem</a></li>
<li><a href="#sec-2-3-2">2.3.2. Multivariate Gaussian</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-3">3. Supervised Learning</a>
<ul>
<li><a href="#sec-3-1">3.1. Function Approximation</a>
<ul>
<li><a href="#sec-3-1-1">3.1.1. Linear Regression</a></li>
</ul>
</li>
<li><a href="#sec-3-2">3.2. Classification</a>
<ul>
<li><a href="#sec-3-2-1">3.2.1. Bayesian Decision Theory</a></li>
<li><a href="#sec-3-2-2">3.2.2. Fisher Linear Discriminant Analysis</a></li>
<li><a href="#sec-3-2-3">3.2.3. Perceptron</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> About This Document</h2>
<div class="outline-text-2" id="text-1">
<p>
This document contains my revision notes for COMP3206 Machine Learning as taught by Professor Mahesan Niranjan at the University of Southampton. This work was produced based on his lectures but any inaccuracies and misunderstandings contained herein are almost certainly my own mistake. This does not represent the whole module, only what I expect to find on the exam.
</p>

<p>
This document my be distributed under the terms of the <a href="https://www.gnu.org/licenses/fdl.html">GNU Free Documentation Licence</a>. A copy of the licence can be found <a href="https://www.gnu.org/licenses/fdl-1.3-standalone.html">here</a>.
</p>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Maths Review</h2>
<div class="outline-text-2" id="text-2">
<p>
This section <b>very</b> briefly reviews some of the maths needed for the rest of the document.
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Matrices and Vectors</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> Basics</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
A vector: 
</p>
\begin{equation*}
\mathbf{x} = \begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_N \end{pmatrix}
\end{equation*}

<p>
A matrix:
</p>
\begin{equation*}
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
\end{equation*}

<p>
Matrix Multiplication:
</p>
\begin{equation*}
[\mathbf{A}\mathbf{B}]_{ij} = \sum_{k=1}^n A_{ik}B_{kj}
\end{equation*}

<p>
Scalar product of vectors: (where &theta; is the angle between the vectors)
</p>
\begin{equation*}
\mathbf{x} \cdot \mathbf{y} = \sum_{i = 1}^{N} x_i y_i = \mathbf{x}^T \mathbf{y} = |\mathbf{x}| |\mathbf{y}| \cos(\theta) 
\end{equation*}

<p>
This can be used to project \(\mathbf{x}\) along the direction \(\mathbf{u}\):
</p>
\begin{equation*}
\mathrm{projection} = \frac{\mathbf{x}^T \mathbf{u}}{|\mathbf{u}|}\mathbf{u}
\end{equation*}

<p>
As an example of matrix algebra: a diagonal matrix is a scalar multiple of the identity matrix. A diagonalizable matrix can be written in erms of a diagonal matrix \(\mathbf{D}\) and some transformation matrix \(\mathbf{S}\) as follows:
</p>
\begin{equation*}
\mathbf{A} = \mathbf{SDS}^{-1} = \mathbf{S}a\mathbf{IS}^{-1}
\end{equation*}

<p>
Using this relationship, a diagonalizable matrix can be efficiently raised to a power using
</p>
\begin{equation*}
\mathbf{A}^n = \mathbf{SD}^n\mathbf{S}^{-1} = \mathbf{S}a^n\mathbf{IS}^{-1}
\end{equation*}

<p>
To prove this by induction for \(n \in \mathbb{N}\):
</p>
\begin{align*}
\mathbf{A}^1 =& \mathbf{SD}^1\mathbf{S}^{-1} \\
\mathrm{Assume } \: \mathbf{A}^n =& \mathbf{SD}^n\mathbf{S}^{-1} \\
\mathbf{A}^{n+1} = \mathbf{A}^n\mathbf{A} =& \mathbf{SD}^n\mathbf{S}^{-1}\mathbf{SD}\mathbf{S}^{-1} \\
                                          =& \mathbf{SD}^n\mathbf{D}\mathbf{S}^{-1} \\
                                          =& \mathbf{SD}^{n+1}\mathbf{S}^{-1} \\
\end{align*}
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Linear Transformation</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
Following from the definition of matrix multiplication, linear transformations on vectors can be represented using a matrix. For example, a vector \(\mathbf{x} \in \mathbb{R}^2\) can be rotated by &theta; by multiplying with a matrix to give a new vector \(\mathbf{r} \in \mathbb{R}^2\). 
</p>
\begin{equation*}
\mathbf{r} = \begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta) \\
\end{pmatrix} \mathbf{x} 
\end{equation*}
\begin{equation*}
|\mathbf{x}| = |\mathbf{r}|
\end{equation*}

<p>
A special case of these linear transformations is to only scale the vector by some amount \(\lambda\):
</p>
\begin{equation*}
\mathbf{Ax} = \lambda \mathbf{x}
\end{equation*}
<p>
For a matrix \(\mathbf{A}\), the vectors \(\mathbf{x}\) and scalars \(\lambda\) which satisfy this equation are called eigen vectors and eigen values, respectively. They can be found by the following method:
</p>
\begin{align*}
\mathbf{Ax} =& \lambda \mathbf{x} \\
\mathbf{Ax} - \lambda \mathbf{x} =& 0 \\
(\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =& 0
\end{align*}
<p>
For non-trivial results:
</p>
\begin{align*}
\mathrm{det}(\mathbf{A} - \lambda\mathbf{I}) = 0
\end{align*}

<p>
As another example of a linear transformation, take a vector \(\mathbf{u} \in \mathbb{R}^d\) and from it construct the matrix \(\mathbf{P} = \mathbf{uu}^T\).
The first interesting property of this matrix is that multiplying it with any vector will make a vector which points in the direction of \(\mathbf{u}\):
</p>
\begin{align*}
\mathbf{Px} = \mathbf{uu}^T\mathbf{x} = \mathbf{u}(\mathbf{u}\cdot\mathbf{x}) = \mathbf{u}a
\end{align*}
<p>
This works because the dot product results in a scalar (written as \(a\)).
</p>

<p>
Another use for \(\mathbf{P}\) is to construct \((2\mathbf{P} - \mathbf{I})\), which will reflect around \(\mathbf{u}\):
</p>
\begin{align*}
(2\mathbf{P} - \mathbf{I})\mathbf{x} &= 2\mathbf{Px} - \mathbf{x} \\
                                     &= 2\mathbf{u}(\mathbf{u}\cdot\mathbf{x}) - \mathbf{x}
\end{align*}
<p>
To see this, it helps to plot it.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Calculus</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Basics</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
For a function \(f\: : \: \mathbb{R}^N \to \mathbb{R}\) 
</p>
\begin{equation*}
\mathbf{\nabla f} (\mathbf{x}) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_N} \\
\end{pmatrix}
\end{equation*}
<p>
\(\mathbf{\nabla}\) should be thought of as the vector gradient of \(f\). 
</p>

<p>
For example, consider \(f(\mathbf{x}) := \mathbf{x}^T\mathbf{Ax}\), \(\mathbf{x} \in \mathbb{R}^2\), where \(\mathbf{A}\) is symmetric (\(A_{21} = A_{12}\))
</p>
\begin{align*}
\mathbf{\nabla f} &= \begin{pmatrix}
\frac{\partial f}{\partial x_1}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\frac{\partial f}{\partial x_2}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\end{pmatrix} \\
&= \begin{pmatrix}
2x_1A_{11} + 2x_2A_{21} \\
2x_2A_{22} + 2x_1A_{21} \\
\end{pmatrix} \\
&= 2\mathbf{Ax}
\end{align*}

<p>
Similarly, \(\mathbf{H}\) (the hessian matrix). This represents the local curvature of a function \(f\: : \: \mathbb{R}^N \to \mathbb{R}\) using second order partial derivatives. The hessian matrix can be defined as
</p>
\begin{equation*}
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{equation*}
</div>
</div>

<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Optimisation</h4>
<div class="outline-text-4" id="text-2-2-2">
</div><ol class="org-ol"><li><a id="sec-2-2-2-1" name="sec-2-2-2-1"></a>Unconstrained<br  /><div class="outline-text-5" id="text-2-2-2-1">
<p>
For example \(\min f(\cdot)\)
</p>

<p>
A commonly used iterative method is gradient decent:
</p>
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \eta \mathbf{\nabla f(\mathbf{x})}
\end{equation*}
<p>
Gradient decent is like taking the path down the gradient to the bottom of the metaphorical hill. 
</p>

<p>
Newton's method converges faster than gradient decent but is also far more computationally intensive
</p>
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \mathbf{H}^{-1}\mathbf{f(\mathbf{x})}
\end{equation*}
</div>
</li>

<li><a id="sec-2-2-2-2" name="sec-2-2-2-2"></a>Constrained - Lagrange Multipliers<br  /><div class="outline-text-5" id="text-2-2-2-2">
<p>
To minimise \(f \: : \: \mathbb{R}^n \to \mathbb{R}\) subject to conditions \(g_i \: : \mathbb{R}^{d_i} \to \mathbb{R} = 0, \: d_i\leq n, \: i = 1, 2, \dots, m\), define \(F \: : \: \mathbb{R}^{n+m} \to \mathbb{R} \: := f(\cdot) - \sum_{i=1}^{N}\lambda_i g_i(\cdot)\). The minimum (or maximum) will be at \(\mathbf{\nabla F} = \mathbf{0}\). 
</p>

<p>
Example:
Minimise \(f(x,y)=xy\) subject to \(x^2+y^2=8\)
</p>
\begin{align*}
&g(x,y) = x^2+y^2-8 \\
&F(x,y,\lambda) = xy - \lambda(x^2+y^2-8) \\
&\mathbf{\nabla F}(x,y,\lambda) = \mathbf{0} \\
&\frac{\partial F}{\partial x} = y - 2\lambda x = 0 \\
&\frac{\partial F}{\partial y} = x - 2\lambda y = 0 \\
&\therefore x = y \\
&\frac{\partial F}{\partial \lambda} = x^2 + y^2 -8 = 0 \\
&\therefore 2x^2 - 8 = 0 \\
&\therefore x = y = \pm 2
\end{align*}

<p>
So there are minima subject to the constraint at \((2,2)\) and \((-2, -2)\).
</p>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Probability</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> Bayes Theorem</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
For \(N\) classes
</p>
\begin{equation*}
P[Y|X] = \frac{P[X|Y]P[Y]}{\sum_{i=1}^{N} P[X|Y_i] P[Y_i]}
\end{equation*}
</div>
</div>

<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> Multivariate Gaussian</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
For a mean \(\mathbf{m} \in \mathbb{R}^p\) and co-variance matrix \(\mathbf{C} \in \mathbb{R}^{p \times p}\)
</p>
\begin{equation*}
p(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^p \mathrm{det}(\mathbf{C})}}\exp\left(-\frac{1}{2}(\mathbf{x} - \mathbf{m})^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m})\right)
\end{equation*}

<p>
The normal (gaussian) distribution can be linearly transformed as so:
</p>
\begin{align*}
 \mathbf{x} \sim& \mathcal{N}(\mathbf{m}, \mathbf{C}) \\
\mathbf{Ax} \sim& \mathcal{N}(\mathbf{Am}, \mathbf{ACA}^T)
\end{align*}
</div>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Supervised Learning</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Function Approximation</h3>
<div class="outline-text-3" id="text-3-1">
</div><div id="outline-container-sec-3-1-1" class="outline-4">
<h4 id="sec-3-1-1"><span class="section-number-4">3.1.1</span> Linear Regression</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
Multivariate linear regression is concerned with learning a function \(f \: : \: \mathbb{R}^p \to \mathbb{R}\) such that the function predicts some useful variable from the inputs. For example, predicting housing prices from the size of the house and it's distance from a school. 
</p>

<p>
In linear regression we use a function of the form
</p>
\begin{equation*}
f(\mathbf{x}) := \mathbf{w}^T\mathbf{x} + w_0
\end{equation*}

<p>
For convenience we will work in \(p+1\) dimensional space:
</p>
\begin{align*}
&\mathbf{y} = (\mathbf{x} \quad 1)^T \\
&\mathbf{a} = (\mathbf{w} \quad w_0)^T \\
&f = \mathbf{y}^T\mathbf{a} \\
\end{align*}

<p>
The function \(f\) forms our model, we have some training data \(\{\mathbf{y}_n, f_n\}_{n=1}^N\) and from it we are trying to learn \(\mathbf{a}\). Alternately, the inputs and outputs can also be written as \(\mathbf{Y}\): an \(N\times(p+1)\) matrix in which the n<sup>th</sup> row is \(\mathbf{y}_n^T\) and \(\mathbf{f}\): an \(N\)-dimensional vector of the outputs.
</p>

<p>
To learn \(\mathbf{a}\), construct a squared distance error function:
</p>
\begin{equation*}
E = \sum_{n=1}^N (\mathbf{y}_n^T\mathbf{a} - f_n)^2 = |\mathbf{Ya}-\mathbf{f}|^2
\end{equation*}

<p>
The error function should be minimised with respect to \(\mathbf{a}\).
</p>
</div>

<ol class="org-ol"><li><a id="sec-3-1-1-1" name="sec-3-1-1-1"></a>Analytic Solution<br  /><div class="outline-text-5" id="text-3-1-1-1">
<p>
As \(E\) is quadratic in \(\mathbf{a}\) and positive valued, we can be sure that the one turning point will be the global minimum. 
</p>

<p>
To find this turning point, equate the differential of \(E\) with respect to \(\mathbf{a}\) to zero.
</p>
\begin{align*}
\mathbf{\nabla_a} E &= 0 \\
                    &= 2\mathbf{Y}^T(\mathbf{Ya} - \mathbf{f}) \\
\therefore \mathbf{Y}^T\mathbf{Ya} &= \mathbf{Y}^T\mathbf{f} \\
\therefore              \mathbf{a} &= (\mathbf{Y}^T\mathbf{Y})^{-1}\mathbf{Y}^T\mathbf{f} \\
\end{align*}

<p>
The problem with this solution is that matrix inversion is an \(O(n^3)\) problem and the dataset used to construct \(\mathbf{Y}\) will typically be very large.
</p>
</div>
</li>

<li><a id="sec-3-1-1-2" name="sec-3-1-1-2"></a>Gradient Decent<br  /><div class="outline-text-5" id="text-3-1-1-2">
<p>
A less computationally expensive method is to use gradient decent: 
</p>

<pre class="example">
Initialise a randomly
Update a[k+1] = a[k] - eta*dE(a)
Until Convergence
</pre>

<p>
Where \(\mathrm{dE}\) is \(\mathbf{\nabla_a}E\) and eta is a small scalar (increasing \(\eta\) leads to faster convergence up to a point, after which the function may not converge at all, decreasing \(\eta\) is slower but is more likely to find the minimum).
</p>
</div>
</li>

<li><a id="sec-3-1-1-3" name="sec-3-1-1-3"></a>Newton's Method<br  /><div class="outline-text-5" id="text-3-1-1-3">
<p>
The solution can also be found using Newton's method. Newton's method will converge a lot faster than gradient decent but computing and inverting \(\mathbf{H}\) could be computationally expensive.
</p>

<pre class="example">
Initialise a randomly
Update a[k+1] = a[k] - eta*invHdE(a)
Until Convergence
</pre>

<p>
Where \(\mathrm{invHdE}\) is \(\mathbf{H}^{-1}\mathbf{\nabla_a}E\).
</p>
</div>
</li>

<li><a id="sec-3-1-1-4" name="sec-3-1-1-4"></a>Regularisation<br  /><div class="outline-text-5" id="text-3-1-1-4">
<p>
The problem of finding \(\mathbf{a}\) my not have a unique solution: it may not be a well-posed problem. Therefore, it is helpful to apply regularisation to constrain the solution. One way of doing this is by using a quadratic regulariser.
</p>

<p>
The quadratic regulariser works by adding an additional term to the error function to constrain the magnitude of \(\mathbf{a}\):
</p>
\begin{equation*}
E = |\mathbf{Ya}-\mathbf{f}|^2 + \gamma|\mathbf{a}|^2
\end{equation*}
<p>
Where \(\gamma\) is a small scalar which controls the trade off between regularisation and fitting the training set.
</p>

<p>
This can then be fed into each solution method we have seen so far. For example the analytical can be obtained as follows:
</p>
\begin{align*}
\mathbf{\nabla_a}E &= 0 \\
                   &= 2\mathbf{Y}^T(\mathbf{Ya}-\mathbf{f})+2\gamma\mathbf{Ia} \\
(\mathbf{Y}^T\mathbf{Y} + \gamma\mathbf{I})\mathbf{a} &= \mathbf{Y}^T\mathbf{f} \\
\therefore \mathbf{a} &= (\mathbf{Y}^T\mathbf{Y}+\gamma\mathbf{I})^{-1}\mathbf{Y}^T\mathbf{f} \\
\end{align*}

<p>
This regularisation may lead to \(\mathbf{a}\) being a sparse matrix. This indicates which variables are actually important.
</p>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Classification</h3>
<div class="outline-text-3" id="text-3-2">
</div><div id="outline-container-sec-3-2-1" class="outline-4">
<h4 id="sec-3-2-1"><span class="section-number-4">3.2.1</span> Bayesian Decision Theory</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
I will only consider gaussian distributed data. This is common because of the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>. The aim is to learn the mean and co-variance for each class. Data can then be assigned to the most probable class. 
</p>

<p>
We assume that we know the classes \(\omega_i, \, i=1,\dots,k\) and class probabilities \(P[\omega_i]\) a priori. 
</p>

<p>
Training data tells us \(p(\mathbf{x} | \omega_i)\). 
</p>

<p>
Formally, the decision rule is to find \(j\) such that 
</p>
\begin{equation*}
\max_j \left(P[\omega_j | \mathbf{x}]\right)
\end{equation*}

<p>
Using Bayes Theorem
</p>
\begin{equation*}
P[\omega_j|\mathbf{x}] = \frac{p(\mathbf{x}|\omega_j)P[\omega_j]}{\sum_{i=1}^{k} P[\mathbf{x}|\omega_i] P[\omega_i]}
\end{equation*}
<p>
The denominator is constant with respect to \(j\) and so is unimportant for the maximum. Therefore the decision rule can be simplified to
</p>
\begin{equation*}
\max_j \left( p(\mathbf{x}|\omega_j)P[\omega_j] \right)
\end{equation*}

<p>
The decision rule can be further simplified. For simplicity I will consider the two class case (\(k=2\)).
</p>

\begin{align*}
p(\mathbf{x}|\omega_1)P[\omega_1] &\lessgtr p(\mathbf{x}|\omega_2)P[\omega_2] \\
\frac{1}{\sqrt{(2\pi)^p\mathrm{det}(\mathbf{C_1})}}\exp\left( -\frac{1}{2}(\mathbf{x} - \mathbf{m}_1)^T\mathbf{C_1}^{-1}(\mathbf{x} - \mathbf{m}_1) \right)P[\omega_1] &\lessgtr  \frac{1}{\sqrt{(2\pi)^p\mathrm{det}(\mathbf{C_2})}}\exp\left( -\frac{1}{2}(\mathbf{x} - \mathbf{m}_2)^T\mathbf{C_2}^{-1}(\mathbf{x} - \mathbf{m}_2) \right)P[\omega_2]
\end{align*}

<p>
From this point we can get a few different classifiers, depending upon the assumptions we make. At first I will assume that the classes share a common co-variance matrix which shows no correlation of the variables (\(\mathbf{C} \propto \mathbf{I}\)) and that the prior probabilites of each class are equal.
</p>

\begin{align*}
(\mathbf{x} - \mathbf{m}_1)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_1) &\lessgtr (\mathbf{x} - \mathbf{m}_2)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_2) \\
(\mathbf{x} - \mathbf{m}_1)^T(\mathbf{x} - \mathbf{m}_1) &\lessgtr (\mathbf{x} - \mathbf{m}_2)^T(\mathbf{x} - \mathbf{m}_2) \\
|\mathbf{x} - \mathbf{m}_1| &\lessgtr |\mathbf{x} - \mathbf{m_2}|
\end{align*}

<p>
This is a distance to mean classifier. To recap, to get to a distance to mean classifier, we had to assume that the variables were multivariate-gaussian distributed, with equal co-variance matrices with no correlation, equal prior class probabilities and distinct means.
</p>

<p>
A slightly more general classifier can be obtained by relaxing the assumptions that the co-variance matrices have no correlation and that the prior probabilities are equal.
</p>
\begin{align*}
(\mathbf{x} - \mathbf{m}_1)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_1) + \log\left(\frac{P[\omega_1]}{P[\omega_2]}\right) &\lessgtr (\mathbf{x} - \mathbf{m}_2)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_2) \\
(\mathbf{x} - \mathbf{m}_1)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_1) -(\mathbf{x} - \mathbf{m}_2)^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m}_2) + \log\left(\frac{P[\omega_1]}{P[\omega_2]}\right) &\lessgtr 0 \\
\mathbf{x}^T\mathbf{C}^{-1}\mathbf{x} -2\mathbf{m_1}^T\mathbf{C}^{-1}\mathbf{x} + \mathbf{m_1}^T\mathbf{C}^{-1}\mathbf{m_1} - \mathbf{x}^T\mathbf{C}^{-1}\mathbf{x} + 2\mathbf{m_2}^T\mathbf{C}^{-1}\mathbf{x} - \mathbf{m_2}^T\mathbf{C}^{-1}\mathbf{m_2} + \log\left(\frac{P[\omega_1]}{P[\omega_2]}\right) &\lessgtr 0 \\
2(\mathbf{m_2} - \mathbf{m_1})^{T}\mathbf{C}^{-1}\mathbf{x} + \left[  \mathbf{m_1}^T\mathbf{C}^{-1}\mathbf{m_1} - \mathbf{m_2}^T\mathbf{C}^{-1}\mathbf{m_2} + \log\left(\frac{P[\omega_1]}{P[\omega_2]}\right) \right] &\lessgtr 0 \\
\end{align*}

<p>
This is also a linear classifier because the assumption that the co-variance matrices are the same allowed the quadratic terms to cancel. This may also be considered as a distance to template classifier (as with the distance to mean classifier) except here we are using Mahalanobis distance instead of euclidean distance.
</p>

<p>
Relaxing that assumption (leaving only the assumption that the data are normally distributed) would lead to a quadratic classifier.
</p>

<p>
A similar application is to calculate the posterior probability of a gaussian distributed variable: (assuming that class 1 is not impossible)
</p>
\begin{align*}
P[\omega_1 | \mathbf{x}] &= \frac{p(\mathbf{x} | \omega_1)P[\omega_1]}{\sum_{i=1}^k p(\mathbf{x} | \omega_i)P[\omega_i]} \\
                         &= \frac{1}{1 + \sum_{i=2}^k \frac{p(\mathbf{x} | \omega_i)P[\omega_i]}{p(\mathbf{x} | \omega_1)P[\omega_1]}} \\
                         &= \frac{1}{1 + \sum_{i=2}^k \frac{P[\omega_i]\sqrt{\mathrm{det}(\mathbf{C}_1)}}{P[\omega_1]\sqrt{\mathrm{det}(\mathbf{C}_i)}}\exp\left[ (\mathbf{x} - \mathbf{m}_i)^T\mathbf{C_i}^{-1}(\mathbf{x} - \mathbf{m}_i) -(\mathbf{x} - \mathbf{m}_1)^T\mathbf{C_1}^{-1}(\mathbf{x} - \mathbf{m}_1) \right]} \\
\end{align*}

<p>
As we saw previously, when the co-variances are equal, the exponential will be linear in \(\mathbf{x}\):
</p>
\begin{align*}
P[\omega_1 | \mathbf{x}] &= \frac{1}{1 + \sum_{i=2}^k \frac{P[\omega_i]\sqrt{\mathrm{det}(\mathbf{C}_1)}}{P[\omega_1]\sqrt{\mathrm{det}(\mathbf{C}_i)}}\exp\left[\mathbf{w}^T\mathbf{x} + \mathbf{w}_0\right]} \\ 
\end{align*}

<p>
For a two class problem, this is an obvious case of a sigmoidal function. For more classes or for a quadratic class boundary (distinct co-variance matrices), the plot still looks intuitively sigmoidal. For example, here is a 3D plot for two classes with different co-variance matrices:
</p>

<div class="figure">
<p><img src="./posteriorProbability3D.png" alt="posteriorProbability3D.png" width="100%" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-3-2-2" class="outline-4">
<h4 id="sec-3-2-2"><span class="section-number-4">3.2.2</span> Fisher Linear Discriminant Analysis</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
The idea behind fisher linear discriminant analysis is to project a higher dimensional problem which is hard to separate onto a lower dimensional surface which has chosen so as to maximise separability.
</p>

<div class="figure">
<p><img src="./projection.jpg" alt="projection.jpg" width="50%" />
</p>
</div>

<p>
Considering the pictured problem of projecting a 2D problem onto 1D, the trick is to pick the gradient \(\mathbf{\omega} \in \mathbb{R}^d\) of the line so as to create maximal separability of classes. This can be captured by the fisher ratio: (recall that the scalar product projects one vector onto another)
</p>
\begin{equation*}
J_F := \frac{(\mathbf{\omega}^T\mathbf{m}_1 - \mathbf{\omega}^T\mathbf{m}_2)^2}{\mathbf{\omega}^T\mathbf{C}_1\mathbf{\omega} + \mathbf{\omega}^T\mathbf{C}_2\mathbf{\omega}}
\end{equation*}
<p>
The fisher ratio can be thought of as the distance of the means divided by the variance on the line. Maximising this will make the points maximally separable because the means will have the greatest distance and the points will have as little spread about the mean as possible.
</p>

<p>
Another way of writing \(J_F\) is as a ratio of quadratic forms
</p>
\begin{align*}
\mathbf{S_B} :=&\, (\mathbf{m}_1 - \mathbf{m}_2)(\mathbf{m}_1-\mathbf{m}_2)^T \\
\mathbf{S_W} :=&\, \mathbf{C}_1 + \mathbf{C}_2 \\
\therefore J_F =&\, \frac{\mathbf{\omega}^T\mathbf{S_B}\mathbf{\omega}}{\mathbf{\omega}^T\mathbf{S_W}\mathbf{\omega}}
\end{align*}

<p>
So to maximise \(J_F\):
</p>
\begin{align*}
\frac{\partial J_F}{\partial \mathbf{\omega}} &= \mathbf{0} \\
                                              &= \frac{2\mathbf{S_B\omega}(\mathbf{\omega}^T\mathbf{S_W\omega}) - 2\mathbf{S_W\omega}(\mathbf{\omega}^T\mathbf{S_B\omega})}{(\mathbf{\omega}^T\mathbf{S_W\omega})^2} \\
\end{align*}

<p>
It is only the direction of \(\mathbf{\omega}\) which matters so we can just combine the scalars:
</p>
\begin{align*}
\mathbf{S_B\omega}-\alpha_0\mathbf{S_W\omega} &= \mathbf{0} \\
\therefore \mathbf{S_W\omega} &= \alpha\mathbf{S_B\omega} \\
\end{align*}

<p>
Note that
</p>
\begin{equation*}
\mathbf{S_B\omega} = (\mathbf{m}_1 - \mathbf{m}_2)(\mathbf{m}_1-\mathbf{m}_2)^T\mathbf{\omega} = (\mathbf{m}_1 - \mathbf{m}_2)\alpha_1
\end{equation*}

<p>
And so points in the same way as \(\mathbf{m}_1 - \mathbf{m}_2\). From this we have an equation for \(\omega\)
</p>
\begin{equation*}
\mathbf{\omega} = \alpha_2(\mathbf{C_1} + \mathbf{C_2})^{-1}(\mathbf{m}_1 - \mathbf{m}_2)
\end{equation*}

<p>
Once projected onto this line, data should be more easily separable using Bayesian Decision Theory.
</p>
</div>
</div>
<div id="outline-container-sec-3-2-3" class="outline-4">
<h4 id="sec-3-2-3"><span class="section-number-4">3.2.3</span> Perceptron</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Perceptron follows a similar process to linear regression except the output is discrete: \(f \: : \: \mathbb{R}^p \to \{1, -1\}\). Our model is 
</p>
\begin{equation*}
f(\mathbf{x}) := 
\begin{cases}
+1 & \mathbf{w}^T\mathbf{x} + w_0 \geq 0 \\
-1 & \mathbf{w}^T\mathbf{x} + w_0 < 0
\end{cases}
\end{equation*}

<p>
Note that the output when the model gives zero is arbitrary.
</p>

<p>
As with linear regression we will use \(\mathbf{w}^T\mathbf{x} + w_0 \equiv \mathbf{y}^T\mathbf{a}\).
</p>

<p>
The intuitive choice for an error function is to count the number of missclassifications. However, this would create an function which steps discretely (looking like stairs). This is piecewise constant and so cannot be differentiated and so is hard too minimise. Instead we will use the the sum of function outputs across the set of unclassified items \(\mathbb{U}\).
</p>
\begin{equation*}
E = -\sum_{\mathbb{y}_n \in \mathbb{U}}\mathbf{y}_n^T\mathbf{a}
\end{equation*}

<p>
This may then be minimised using stochastic gradient decent. First the derivative: 
</p>
\begin{equation*}
\mathbf{\nabla_a}E = -\sum_{\mathbb{y}_n \in \mathbb{U}}\mathbf{y}_n
\end{equation*}

<p>
Therefore, using randomly chosen \(\mathbf{y}_n\) we can update like this
</p>
\begin{equation*}
\mathbf{a}^{(k+1)} = \mathbf{a}^{(k)} + \mathbf{y}_n
\end{equation*}
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Tom Eccles</p>
<p class="date">Created: 2017-01-10 Tue 10:17</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
