<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Machine Learning Revision</title>
<!-- 2017-01-05 Thu 13:47 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Tom Eccles" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Machine Learning Revision</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. About This Document</a></li>
<li><a href="#sec-2">2. Maths Review</a>
<ul>
<li><a href="#sec-2-1">2.1. Matrices and Vectors</a>
<ul>
<li><a href="#sec-2-1-1">2.1.1. Basics</a></li>
<li><a href="#sec-2-1-2">2.1.2. Linear Transformation</a></li>
</ul>
</li>
<li><a href="#sec-2-2">2.2. Calculus</a>
<ul>
<li><a href="#sec-2-2-1">2.2.1. Basics</a></li>
<li><a href="#sec-2-2-2">2.2.2. Optimisation</a></li>
</ul>
</li>
<li><a href="#sec-2-3">2.3. Probability</a>
<ul>
<li><a href="#sec-2-3-1">2.3.1. Bayes Theorem</a></li>
<li><a href="#sec-2-3-2">2.3.2. Multivariate Gaussian</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> About This Document</h2>
<div class="outline-text-2" id="text-1">
<p>
This document contains my revision notes for COMP3206 Machine Learning as taught by Professor Mahesan Niranjan at the University of Southampton. This work was produced based on his lectures but any inaccuracies and misunderstandings contained herein are almost certainly my own mistake.
</p>

<p>
This document my be distributed under the terms of the <a href="https://www.gnu.org/licenses/fdl.html">GNU Free Documentation Licence</a>. A copy of the licence can be found <a href="https://www.gnu.org/licenses/fdl-1.3-standalone.html">here</a>.
</p>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Maths Review</h2>
<div class="outline-text-2" id="text-2">
<p>
This section <b>very</b> briefly reviews some of the maths needed for the rest of the document.
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Matrices and Vectors</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> Basics</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
A vector: 
</p>
\begin{equation*}
\mathbf{x} = \begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_N \end{pmatrix}
\end{equation*}

<p>
A matrix:
</p>
\begin{equation*}
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
\end{equation*}

<p>
Matrix Multiplication:
</p>
\begin{equation*}
[\mathbf{A}\mathbf{B}]_{ij} = \sum_{k=1}^n A_{ik}B_{kj}
\end{equation*}

<p>
Scalar product of vectors: (where &theta; is the angle between the vectors)
</p>
\begin{equation*}
\mathbf{x} \cdot \mathbf{y} = \sum_{i = 1}^{N} x_i y_i = \mathbf{x}^T \mathbf{y} = |\mathbf{x}| |\mathbf{y}| \cos(\theta) 
\end{equation*}

<p>
This can be used to project \(\mathbf{x}\) along the direction \(\mathbf{u}\):
</p>
\begin{equation*}
\mathrm{projection} = \frac{\mathbf{x}^T \mathbf{u}}{|\mathbf{u}|}\mathbf{u}
\end{equation*}

<p>
As an example of matrix algebra: a diagonal matrix is a scalar multiple of the identity matrix. A diagonalizable matrix can be written in erms of a diagonal matrix \(\mathbf{D}\) and some transformation matrix \(\mathbf{S}\) as follows:
</p>
\begin{equation*}
\mathbf{A} = \mathbf{SDS}^{-1} = \mathbf{S}a\mathbf{IS}^{-1}
\end{equation*}

<p>
Using this relationship, a diagonalizable matrix can be efficiently raised to a power using
</p>
\begin{equation*}
\mathbf{A}^n = \mathbf{SD}^n\mathbf{S}^{-1} = \mathbf{S}a^n\mathbf{IS}^{-1}
\end{equation*}

<p>
To prove this by induction for \(n \in \mathbb{N}\):
</p>
\begin{align*}
\mathbf{A}^1 =& \mathbf{SD}^1\mathbf{S}^{-1} \\
\mathrm{Assume } \: \mathbf{A}^n =& \mathbf{SD}^n\mathbf{S}^{-1} \\
\mathbf{A}^{n+1} = \mathbf{A}^n\mathbf{A} =& \mathbf{SD}^n\mathbf{S}^{-1}\mathbf{SD}\mathbf{S}^{-1} \\
                                          =& \mathbf{SD}^n\mathbf{D}\mathbf{S}^{-1} \\
                                          =& \mathbf{SD}^{n+1}\mathbf{S}^{-1} \\
\end{align*}
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Linear Transformation</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
Following from the definition of matrix multiplication, linear transformations on vectors can be represented using a matrix. For example, a vector \(\mathbf{x} \in \mathbb{R}^2\) can be rotated by &theta; by multiplying with a matrix to give a new vector \(\mathbf{r} \in \mathbb{R}^2\). 
</p>
\begin{equation*}
\mathbf{r} = \begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta) \\
\end{pmatrix} \mathbf{x} 
\end{equation*}
\begin{equation*}
|\mathbf{x}| = |\mathbf{r}|
\end{equation*}

<p>
A special case of these linear transformations is to only scale the vector by some amount \(\lambda\):
</p>
\begin{equation*}
\mathbf{Ax} = \lambda \mathbf{x}
\end{equation*}
<p>
For a matrix \(\mathbf{A}\), the vectors \(\mathbf{x}\) and scalars \(\lambda\) which satisfy this equation are called eigen vectors and eigen values, respectively. They can be found by the following method:
</p>
\begin{align*}
\mathbf{Ax} =& \lambda \mathbf{x} \\
\mathbf{Ax} - \lambda \mathbf{x} =& 0 \\
(\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =& 0
\end{align*}
<p>
For non-trivial results:
</p>
\begin{align*}
\mathrm{det}(\mathbf{A} - \lambda\mathbf{I}) = 0
\end{align*}

<p>
As another example of a linear transformation, take a vector \(\mathbf{u} \in \mathbb{R}^d\) and from it construct the matrix \(\mathbf{P} = \mathbf{uu}^T\).
The first interesting property of this matrix is that multiplying it with any vector will make a vector which points in the direction of \(\mathbf{u}\):
</p>
\begin{align*}
\mathbf{Px} = \mathbf{uu}^T\mathbf{x} = \mathbf{u}(\mathbf{u}\cdot\mathbf{x}) = \mathbf{u}a
\end{align*}
<p>
This works because the dot product results in a scalar (written as \(a\)).
</p>

<p>
Another use for \(\mathbf{P}\) is to construct \((2\mathbf{P} - \mathbf{I})\), which will reflect around \(\mathbf{u}\):
</p>
\begin{align*}
(2\mathbf{P} - \mathbf{I})\mathbf{x} &= 2\mathbf{Px} - \mathbf{x} \\
                                     &= 2\mathbf{u}(\mathbf{u}\cdot\mathbf{x}) - \mathbf{x}
\end{align*}
<p>
To see this, it helps to plot it.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Calculus</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Basics</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
For a function \(f\: : \: \mathbb{R}^N \to \mathbb{R}\) 
</p>
\begin{equation*}
\mathbf{\nabla f} (\mathbf{x}) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_N} \\
\end{pmatrix}
\end{equation*}
<p>
\(\mathbf{\nabla}\) should be thought of as the vector gradient of \(f\). 
</p>

<p>
For example, consider \(f(\mathbf{x}) := \mathbf{x}^T\mathbf{Ax}\), \(\mathbf{x} \in \mathbb{R}^2\), where \(\mathbf{A}\) is symmetric (\(A_{21} = A_{12}\))
</p>
\begin{align*}
\mathbf{\nabla f} &= \begin{pmatrix}
\frac{\partial f}{\partial x_1}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\frac{\partial f}{\partial x_2}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\end{pmatrix} \\
&= \begin{pmatrix}
2x_1A_{11} + 2x_2A_{21} \\
2x_2A_{22} + 2x_1A_{21} \\
\end{pmatrix} \\
&= 2\mathbf{Ax}
\end{align*}

<p>
Similarly, \(\mathbf{H}\) (the hessian matrix). This represents the local curvature of a function \(f\: : \: \mathbb{R}^N \to \mathbb{R}\) using second order partial derivatives. The hessian matrix can be defined as
</p>
\begin{equation*}
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{equation*}
</div>
</div>

<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Optimisation</h4>
<div class="outline-text-4" id="text-2-2-2">
</div><ol class="org-ol"><li><a id="sec-2-2-2-1" name="sec-2-2-2-1"></a>Unconstrained<br  /><div class="outline-text-5" id="text-2-2-2-1">
<p>
For example \(\min f(\cdot)\)
</p>

<p>
A commonly used iterative method is gradient decent:
</p>
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \eta \mathbf{\nabla f(\mathbf{x})}
\end{equation*}
<p>
Gradient decent is like taking the path down the gradient to the bottom of the metaphorical hill. 
</p>

<p>
Newton's method converges faster than gradient decent but is also far more computationally intensive
</p>
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \mathbf{H}^{-1}\mathbf{f(\mathbf{x})}
\end{equation*}
</div>
</li>

<li><a id="sec-2-2-2-2" name="sec-2-2-2-2"></a>Constrained - Lagrange Multipliers<br  /><div class="outline-text-5" id="text-2-2-2-2">
<p>
To minimise \(f \: : \: \mathbb{R}^n \to \mathbb{R}\) subject to conditions \(g_i \: : \mathbb{R}^{d_i} \to \mathbb{R} = 0, \: d_i\leq n, \: i = 1, 2, \dots, m\), define \(F \: : \: \mathbb{R}^{n+m} \to \mathbb{R} \: := f(\cdot) - \sum_{i=1}^{N}\lambda_i g_i(\cdot)\). The minimum (or maximum) will be at \(\mathbf{\nabla F} = \mathbf{0}\). 
</p>

<p>
Example:
Minimise \(f(x,y)=xy\) subject to \(x^2+y^2=8\)
</p>
\begin{align*}
&g(x,y) = x^2+y^2-8 \\
&F(x,y,\lambda) = xy - \lambda(x^2+y^2-8) \\
&\mathbf{\nabla F}(x,y,\lambda) = \mathbf{0} \\
&\frac{\partial F}{\partial x} = y - 2\lambda x = 0 \\
&\frac{\partial F}{\partial y} = x - 2\lambda y = 0 \\
&x = y \\
&\frac{\partial F}{\partial \lambda} = x^2 + y^2 -8 = 0 \\
&2x^2 - 8 = 0 \\
&x = y = \pm 2
\end{align*}

<p>
So there are minima subject to the constraint at \((2,2)\) and \((-2, -2)\).
</p>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Probability</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> Bayes Theorem</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
For \(N\) classes
</p>
\begin{equation*}
P[Y|X] = \frac{P[X|Y]P[Y]}{\sum_{i=1}^{N} P[X|Y_i] P[Y_i]}
\end{equation*}
</div>
</div>

<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> Multivariate Gaussian</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
For a mean \(\mathbf{m} \in \mathbb{R}^p\) and co-variance matrix \(\mathbf{C} \in \mathbb{R}^{p \times p}\)
</p>
\begin{equation*}
p(\mathbf{x}) = \frac{1}{(2\pi)^p \mathrm{det}(\mathbf{C})^{1/2}}\left(\frac{1}{2}(\mathbf{x} - \mathbf{m})^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m})\right)
\end{equation*}

<p>
The normal (gaussian) distribution can be linearly transformed as so:
</p>
\begin{align*}
 \mathbf{x} \sim& \mathcal{N}(\mathbf{m}, \mathbf{C}) \\
\mathbf{Ax} \sim& \mathcal{N}(\mathbf{Am}, \mathbf{ACA}^T)
\end{align*}
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Tom Eccles</p>
<p class="date">Created: 2017-01-05 Thu 13:47</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.4.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
