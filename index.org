#+TITLE: Machine Learning Revision
#+AUTHOR: Tom Eccles
#+LATEX_HEADER: \usepackage{amsmath}

* About This Document
This document contains my revision notes for COMP3206 Machine Learning as taught by Professor Mahesan Niranjan at the University of Southampton. This work was produced based on his lectures but any inaccuracies and misunderstandings contained herein are almost certainly my own mistake.

This document my be distributed under the terms of the [[https://www.gnu.org/licenses/fdl.html][GNU Free Documentation Licence]]. A copy of the licence can be found [[https://www.gnu.org/licenses/fdl-1.3-standalone.html][here]].
* Maths Review
This section *very* briefly reviews some of the maths needed for the rest of the document.

** Matrices and Vectors
*** Basics
A vector: 
\begin{equation*}
\mathbf{x} = \begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_N \end{pmatrix}
\end{equation*}

A matrix:
\begin{equation*}
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
\end{equation*}

Matrix Multiplication:
\begin{equation*}
[\mathbf{A}\mathbf{B}]_{ij} = \sum_{k=1}^n A_{ik}B_{kj}
\end{equation*}

Scalar product of vectors: (where \theta is the angle between the vectors)
\begin{equation*}
\mathbf{x} \cdot \mathbf{y} = \sum_{i = 1}^{N} x_i y_i = \mathbf{x}^T \mathbf{y} = |\mathbf{x}| |\mathbf{y}| \cos(\theta) 
\end{equation*}

This can be used to project $\mathbf{x}$ along the direction $\mathbf{u}$:
\begin{equation*}
\mathrm{projection} = \frac{\mathbf{x}^T \mathbf{u}}{|\mathbf{u}|}\mathbf{u}
\end{equation*}

As an example of matrix algebra: a diagonal matrix is a scalar multiple of the identity matrix. A diagonalizable matrix can be written in erms of a diagonal matrix $\mathbf{D}$ and some transformation matrix $\mathbf{S}$ as follows:
\begin{equation*}
\mathbf{A} = \mathbf{SDS}^{-1} = \mathbf{S}a\mathbf{IS}^{-1}
\end{equation*}

Using this relationship, a diagonalizable matrix can be efficiently raised to a power using
\begin{equation*}
\mathbf{A}^n = \mathbf{SD}^n\mathbf{S}^{-1} = \mathbf{S}a^n\mathbf{IS}^{-1}
\end{equation*}

To prove this by induction for $n \in \mathbb{N}$:
\begin{align*}
\mathbf{A}^1 =& \mathbf{SD}^1\mathbf{S}^{-1} \\
\mathrm{Assume } \: \mathbf{A}^n =& \mathbf{SD}^n\mathbf{S}^{-1} \\
\mathbf{A}^{n+1} = \mathbf{A}^n\mathbf{A} =& \mathbf{SD}^n\mathbf{S}^{-1}\mathbf{SD}\mathbf{S}^{-1} \\
                                          =& \mathbf{SD}^n\mathbf{D}\mathbf{S}^{-1} \\
					  =& \mathbf{SD}^{n+1}\mathbf{S}^{-1} \\
\end{align*}

*** Linear Transformation
Following from the definition of matrix multiplication, linear transformations on vectors can be represented using a matrix. For example, a vector $\mathbf{x} \in \mathbb{R}^2$ can be rotated by \theta by multiplying with a matrix to give a new vector $\mathbf{r} \in \mathbb{R}^2$. 
\begin{equation*}
\mathbf{r} = \begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta) \\
\end{pmatrix} \mathbf{x} 
\end{equation*}
\begin{equation*}
|\mathbf{x}| = |\mathbf{r}|
\end{equation*}

A special case of these linear transformations is to only scale the vector by some amount $\lambda$:
\begin{equation*}
\mathbf{Ax} = \lambda \mathbf{x}
\end{equation*}
For a matrix $\mathbf{A}$, the vectors $\mathbf{x}$ and scalars $\lambda$ which satisfy this equation are called eigen vectors and eigen values, respectively. They can be found by the following method:
\begin{align*}
\mathbf{Ax} =& \lambda \mathbf{x} \\
\mathbf{Ax} - \lambda \mathbf{x} =& 0 \\
(\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =& 0
\end{align*}
For non-trivial results:
\begin{align*}
\mathrm{det}(\mathbf{A} - \lambda\mathbf{I}) = 0
\end{align*}

As another example of a linear transformation, take a vector $\mathbf{u} \in \mathbb{R}^d$ and from it construct the matrix $\mathbf{P} = \mathbf{uu}^T$.
The first interesting property of this matrix is that multiplying it with any vector will make a vector which points in the direction of $\mathbf{u}$:
\begin{align*}
\mathbf{Px} = \mathbf{uu}^T\mathbf{x} = \mathbf{u}(\mathbf{u}\cdot\mathbf{x}) = \mathbf{u}a
\end{align*}
This works because the dot product results in a scalar (written as $a$).

Another use for $\mathbf{P}$ is to construct $(2\mathbf{P} - \mathbf{I})$, which will reflect around $\mathbf{u}$:
\begin{align*}
(2\mathbf{P} - \mathbf{I})\mathbf{x} &= 2\mathbf{Px} - \mathbf{x} \\
                                     &= 2\mathbf{u}(\mathbf{u}\cdot\mathbf{x}) - \mathbf{x}
\end{align*}
To see this, it helps to plot it.

** Calculus
*** Basics
For a function $f\: : \: \mathbb{R}^N \to \mathbb{R}$ 
\begin{equation*}
\mathbf{\nabla f} (\mathbf{x}) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_N} \\
\end{pmatrix}
\end{equation*}
$\mathbf{\nabla}$ should be thought of as the vector gradient of $f$. 

For example, consider $f(\mathbf{x}) := \mathbf{x}^T\mathbf{Ax}$, $\mathbf{x} \in \mathbb{R}^2$, where $\mathbf{A}$ is symmetric ($A_{21} = A_{12}$)
\begin{align*}
\mathbf{\nabla f} &= \begin{pmatrix}
\frac{\partial f}{\partial x_1}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\frac{\partial f}{\partial x_2}\left( x_1^2A_{11} + 2x_1x_2A_{21} + x_2^2A_{22} \right) \\
\end{pmatrix} \\
&= \begin{pmatrix}
2x_1A_{11} + 2x_2A_{21} \\
2x_2A_{22} + 2x_1A_{21} \\
\end{pmatrix} \\
&= 2\mathbf{Ax}
\end{align*}

Similarly, $\mathbf{H}$ (the hessian matrix). This represents the local curvature of a function $f\: : \: \mathbb{R}^N \to \mathbb{R}$ using second order partial derivatives. The hessian matrix can be defined as
\begin{equation*}
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{equation*}

*** Optimisation
**** Unconstrained
For example $\min f(\cdot)$

A commonly used iterative method is gradient decent:
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \eta \mathbf{\nabla f(\mathbf{x})}
\end{equation*}
Gradient decent is like taking the path down the gradient to the bottom of the metaphorical hill. 

Newton's method converges faster than gradient decent but is also far more computationally intensive
\begin{equation*}
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} - \mathbf{H}^{-1}\mathbf{f(\mathbf{x})}
\end{equation*}

**** Constrained - Lagrange Multipliers
To minimise $f \: : \: \mathbb{R}^n \to \mathbb{R}$ subject to conditions $g_i \: : \mathbb{R}^{d_i} \to \mathbb{R} = 0, \: d_i\leq n, \: i = 1, 2, \dots, m$, define $F \: : \: \mathbb{R}^{n+m} \to \mathbb{R} \: := f(\cdot) - \sum_{i=1}^{N}\lambda_i g_i(\cdot)$. The minimum (or maximum) will be at $\mathbf{\nabla F} = \mathbf{0}$. 

Example:
Minimise $f(x,y)=xy$ subject to $x^2+y^2=8$
\begin{align*}
&g(x,y) = x^2+y^2-8 \\
&F(x,y,\lambda) = xy - \lambda(x^2+y^2-8) \\
&\mathbf{\nabla F}(x,y,\lambda) = \mathbf{0} \\
&\frac{\partial F}{\partial x} = y - 2\lambda x = 0 \\
&\frac{\partial F}{\partial y} = x - 2\lambda y = 0 \\
&x = y \\
&\frac{\partial F}{\partial \lambda} = x^2 + y^2 -8 = 0 \\
&2x^2 - 8 = 0 \\
&x = y = \pm 2
\end{align*}

So there are minima subject to the constraint at $(2,2)$ and $(-2, -2)$.

** Probability
*** Bayes Theorem
For $N$ classes
\begin{equation*}
P[Y|X] = \frac{P[X|Y]P[Y]}{\sum_{i=1}^{N} P[X|Y_i] P[Y_i]}
\end{equation*}

*** Multivariate Gaussian
For a mean $\mathbf{m} \in \mathbb{R}^p$ and co-variance matrix $\mathbf{C} \in \mathbb{R}^{p \times p}$
\begin{equation*}
p(\mathbf{x}) = \frac{1}{(2\pi)^p \mathrm{det}(\mathbf{C})^{1/2}}\left(\frac{1}{2}(\mathbf{x} - \mathbf{m})^T\mathbf{C}^{-1}(\mathbf{x} - \mathbf{m})\right)
\end{equation*}

The normal (gaussian) distribution can be linearly transformed as so:
\begin{align*}
 \mathbf{x} \sim& \mathcal{N}(\mathbf{m}, \mathbf{C}) \\
\mathbf{Ax} \sim& \mathcal{N}(\mathbf{Am}, \mathbf{ACA}^T)
\end{align*}

